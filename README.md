# NewLeadsIdentifier

This is a fully-automated LinkedIn crawler that helps identifying companies interested in a certain product by crawling the related-job offers. Currently (January 2020), it works like a charm. I am planning to maintain the code over time: should LinkedIn change its structure, I will update the scripts accordingly. 

## Disclaimer 
This solution is intended for educational purposes only. Because of its crawling-nature, NewLeadsIdentifier goes against Linkedin's User Agreement (see Section 8.2) and might result in a temporary/permanent ban of the account used for scraping company information; on the other hand no personal data are stored and all the retrieved information are publicly available on the website (even before authentication). Use at your discretion, I decline all responsibility.

## Use Case (Example)
As a SFDC professional, I might find useful to know what companies are adopting SFDC. Job offers can help identifying them. The assumption is: if a certain company requires their potential employees to be able to work with SFDC, one can assume with a reasonable degree of confidence SFDC _must_ be adopted within the company. Consequently, by scraping companies that posted SFDC-related jobs it is possible to get a list of prospects potentially interested in SFDC-related solutions/offers. 
Of course, some additional clean-ups would still be required (ex. recruting companies might post SFDC-related job offers without using SFDC themselves) but wrong entries can be easily filtered out at a later stage.

## Usage 
The current solution is built around the use case described above but it can be easily tailored around different products by changing the search URL in retrieveurls.py. 
```bash
https://www.linkedin.com/jobs/search/?keywords=SFDC&sortBy=DD&start={}
```
Make sure not to remove the paging or else only the search results from the very first page will be considered.

On both scripts (retrieveurls.py & parseprofiles.py) you will also need to provide the Chromedriver path and the LinkedIn credentials of the crawling user (I used Selenium for the automation). 

Run the scripts (in the following order) only when all the changes are correctly applied:
1) retrieveurls.py: this generates a list of companies that posted job ads (based on the search URL)
2) parseprofiles.py: this crawls the URLs generated by retrieveurls.py and parse the company information from each LinkedIn profile

The following information will be stored in a .csv file (if available in the company profile):

- ID (I am using the universal name as reference, not the actual numeric ID: LinkedIn uses both)
- Company name
- Short description
- Industry (useful to clean-up the undesired results, see the "Use Case")
- Corporate website
- Phone
- Company size (number of employees)
- HQ
- Year of foundation 
- Specialties (this information can be also leveraged to filter out the undesired results)
- Link to the LinkedIn profile page

